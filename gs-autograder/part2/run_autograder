#!/usr/bin/env python3

import json
import subprocess
import traceback
from datetime import datetime
import os.path as path
import os
import glob
import shutil
import re

# error message to show to student when an unexpected error has occurred
UNEXPECTED = "unexpected error; contact the TA with a link to this page"

SUBMISSION_DIR = "/autograder/submission"
SOURCE_DIR = "/autograder/source"
RESULTS_DIR = "/autograder/results"
END_RESULTS = f"{RESULTS_DIR}/results.json"

TEST1_PASS = False
TEST1_TIMEOUT = False

TEST2_PASS = False
TEST2_TIMEOUT = False


class SubmissionError(Exception):
    def __init__(self, msg, extra_msg=""):
        self.msg = msg
        self.extra_msg = extra_msg


def main():
    global TEST1_PASS
    global TEST1_TIMEOUT
    global TEST2_PASS
    global TEST2_TIMEOUT

    print(datetime.now())
    print()
    print("begin setup")
    print()

    # Check if the required files exist
    output = ""
    output += "Expecting to find the following files:\n"

    client_exists = path.exists(f"{SUBMISSION_DIR}/src/client/MyDBClient.java")
    output += "src/client/MyDBClient.java ... "
    output += "found" if client_exists else "NOT FOUND"
    output += "\n"

    server_exists = path.exists(
        f"{SUBMISSION_DIR}/src/server/MyDBSingleServer.java")
    output += "src/server/MyDBSingleServer.java ... "
    output += "found" if server_exists else "NOT FOUND"
    output += "\n"

    server_exists = path.exists(
        f"{SUBMISSION_DIR}/src/server/MyDBReplicatedServer.java")
    output += "src/server/MyDBReplicatedServer.java ... "
    output += "found" if server_exists else "NOT FOUND"
    output += "\n"

    document_exists = path.exists(f"{SUBMISSION_DIR}/Design.pdf")
    output += "Design.pdf ... "
    output += "found" if document_exists else "NOT FOUND"
    output += "\n"

    if not (client_exists and server_exists):
        raise SubmissionError(output)

    # ============================================================
    # Reset the source directory
    output += "Resetting all other files to their original form\n"
    run(f"rm {SOURCE_DIR}/framework/src/client/MyDBClient.java")
    run(f"rm {SOURCE_DIR}/framework/src/server/MyDBSingleServer.java")
    run(f"rm {SOURCE_DIR}/framework/src/server/MyDBReplicatedServer.java")
    
    run(f"mv src/client/MyDBClient.java {SOURCE_DIR}/framework/src/client/MyDBClient.java")
    run(f"mv src/server/MyDBSingleServer.java {SOURCE_DIR}/framework/src/server/MyDBSingleServer.java")
    run(f"mv src/server/MyDBReplicatedServer.java {SOURCE_DIR}/framework/src/server/MyDBReplicatedServer.java")

    try:
        for item in os.listdir(f"{SUBMISSION_DIR}/src"):
            s = os.path.join(f"{SUBMISSION_DIR}/src", item)
            d = os.path.join(f"{SOURCE_DIR}/framework/src", item)
            if os.path.isdir(s):
                shutil.copytree(s, d, dirs_exist_ok=True)
    except:
        pass

    run("rm -r *")
    run(f"cp -r {SOURCE_DIR}/framework/* .")  # copy all files from source dir to submission dir

    # ============================================================
    # Complie the code

    output += "Compiling...\n"
    print()
    print("compiling...")
    print()

    jar_files = ["lib/" + x for x in os.listdir(f"{SUBMISSION_DIR}/lib")]
    java_files = glob.glob(f"{SUBMISSION_DIR}/src/**/*.java", recursive=True)
    test_java_files = [
    	"test/GraderSingleServer.java",
    	"test/GraderSingleServerGradescope.java",
        "test/GraderGradescope.java",
        "test/Grader.java"
        ]
    java_files += test_java_files

    
    print(f"jar_files: {jar_files}")
    print(f"java_files: {java_files}")

    compile_proc = run(
        f"javac -cp {':'.join(jar_files)} {' '.join(java_files)}",
        capture_output=True
    )
    compile_proc_stdout = compile_proc.stdout.decode('utf-8')
    compile_proc_stderr = compile_proc.stderr.decode('utf-8')

    print(compile_proc.args)
    print("compilation stdout:")
    print(compile_proc_stdout)
    print()
    print("compilation stderr:")
    print(compile_proc_stderr)
    print()

    if compile_proc.returncode != 0:
        output += "Java compilation error:\n"
        output += compile_proc_stderr
        raise SubmissionError(output)

    output += "Compilation successful\n"

    output += "All expected .java files found and successfully compiled. "
    # ============================================================

    # ============================================================
    # Start the Cassandra node
    print("Starting Cassandra...")
    output += "Starting cassandra...\n"
    run("/autograder/source/before_run_autograder.sh")
    # ============================================================

    # ============================================================
    # Run the public tests
    output += "Running Tests...\n"
    print()
    print("Running public unit tests...")
    print()


    run_proc = run(
        f"java -cp src:test:{':'.join(jar_files)} GraderGradescope",
        capture_output=True,
        timeout=90 
    )

    run_proc_stdout = run_proc.stdout.decode('utf-8')
    run_proc_stderr = run_proc.stderr.decode('utf-8')

    output += "Public unit tests finished running.\n"
    print("Public unit tests finished running.")

    print(run_proc.args)
    print("runtime stdout:")
    print(run_proc_stdout)
    print()
    print("runtime stderr:")
    print(run_proc_stderr)
    print()
    # ============================================================

    # ============================================================
    # Calculate the score

    # Parse stdout to get test names and their max scores
    TestNameAndScore = {}
    for line in run_proc_stdout.strip().split("\n"):
        match = re.search(r"testName=(\w+)\smax_score=([\d\.]+)", line)
        if match:
            test_name = match.group(1)
            max_score = float(match.group(2))
            TestNameAndScore[test_name] = {"max_score": max_score}
    TestNameAndScore = dict(sorted(TestNameAndScore.items(), key=lambda x: x[0]))

    test_score = 0.0
    test1_autograder_score = 25.0
    test1_max_score = 0.0

    # if a timeout occurs 
    if run_proc.returncode == -1:
        print("A timeout has occurred when running the tests.")
        print("Deducting 1 point for TIMEOUT error (probably not terminating cleanly).")
        test1_autograder_score -= 1.0
        print(f"{test1_autograder_score} points remain")
        TEST1_TIMEOUT = True
    
    # Get the total max score
    for test_name, test_info in TestNameAndScore.items():
        test1_max_score += test_info["max_score"]

    # If any error occurs
    if run_proc.returncode != 0: 
        output += "Java runtime error\n"
        # raise SubmissionError(output)
        failed_test_name, failed_test_num = capture_failed_test(run_proc_stderr)
        print(f"The submission failed at : {failed_test_name}")

        # Sum up the scores before the failed test 
        for test_name, test_info in TestNameAndScore.items(): 
            if test_name == failed_test_name:
                break
            received_score = test_info["max_score"]
            print(f"testname: {test_name}; get {received_score } points")
            test_score += received_score 
        
    # If all tests pass
    else:
        failed_test_name = None
        failed_test_num = None
        TEST1_PASS = True
        test_score = test1_max_score

    test1_final_score = (test_score / test1_max_score) * test1_autograder_score

    # If any exception occurs
    if capture_exception(run_proc_stderr):
        output += "Exception:\n"
        output += capture_exception(run_proc_stderr)
        output += "\n"
        print("Stderr output indicates unexpected exceptions (e.g., ArrayOutofBounds, NullPointer or others) because of inadequate checks in code.")
        print(f"Lose 1 point out of {test1_autograder_score} ({test1_autograder_score - 1} points remain)")
        test1_final_score -= 1.0
        if test1_final_score < 0:  # score cannot be negative
            test1_final_score = 0.0

    output += f"Public test final score: {test1_final_score}\n"
    print(f"You received {test_score} out of {test1_max_score} in the public tests")
    print(f"Public test final_score: {test1_final_score} out of {test1_autograder_score}")
    # ============================================================

    # ============================================================
    # Run the private tests
    output += "Running Private Tests...\n"
    print()
    print("Running private unit tests...")
    print()


    run_proc = run(
        f"java -cp src:test:{':'.join(jar_files)} -Dconfig={SOURCE_DIR}/framework/conf/servers_20.properties GraderGradescope",
        capture_output=True,
        timeout=90 
    )

    run_proc_stdout = run_proc.stdout.decode('utf-8')
    run_proc_stderr = run_proc.stderr.decode('utf-8')

    output += "Private unit tests finished running.\n"
    print("Private unit tests finished running.")

    print(run_proc.args)
    print("runtime stdout:")
    print(run_proc_stdout)
    print()
    print("runtime stderr:")
    print(run_proc_stderr)
    print()

    # keep the stdout and stderr in test 1
    run_proc_stdout_test1 = run_proc_stdout
    run_proc_stderr_test1 = run_proc_stderr
    # ============================================================

    # ============================================================
    TestNameAndScore = {}
    for line in run_proc_stdout.strip().split("\n"):
        match = re.search(r"testName=(\w+)\smax_score=([\d\.]+)", line)
        if match:
            test_name = match.group(1)
            max_score = float(match.group(2))
            TestNameAndScore[test_name] = {"max_score": max_score}
    TestNameAndScore = dict(sorted(TestNameAndScore.items(), key=lambda x: x[0]))

    test_score = 0.0
    test2_autograder_score = 30.0
    test2_max_score = 0.0

    # if a timeout occurs 
    if run_proc.returncode == -1:
        print("A timeout has occurred when running the tests.")
        if TEST1_TIMEOUT:
            print("Have deducted 1 point for TIMEOUT error in the public tests.")
            print("Be lanient int the private test. No further deduction.")
        else:
            print("Deducting 1 point for TIMEOUT error (probably not terminating cleanly).")
            test2_autograder_score -= 1.0
            print(f"{test2_autograder_score} points remain")
            TEST2_TIMEOUT = True
    
    # Get the total max score in this test
    for test_name, test_info in TestNameAndScore.items():
        test2_max_score += test_info["max_score"]

    # If any error occurs
    if run_proc.returncode != 0: 
        output += "Java runtime error\n"
        failed_test_name, failed_test_num = capture_failed_test(run_proc_stderr)
        print(f"The submission failed at : {failed_test_name}")
        # Sum up the scores before the failed test 
        for test_name, test_info in TestNameAndScore.items(): 
            if test_name == failed_test_name:
                break
            received_score = test_info["max_score"]
            print(f"testname: {test_name}; get {received_score } points")
            test_score += received_score 
        
    # If all tests pass
    else:
        failed_test_name = None
        failed_test_num = None
        TEST2_PASS = True
        test_score = test2_max_score

    test2_final_score = (test_score / test2_max_score) * test2_autograder_score

    # If any exception occurs
    if capture_exception(run_proc_stderr):
        output += "Exception:\n"
        output += capture_exception(run_proc_stderr)
        output += "\n"
        print("Stderr output indicates unexpected exceptions (e.g., ArrayOutofBounds, NullPointer or others) because of inadequate checks in code.")
        print(f"Lose 1 point out of {test2_autograder_score} ({test2_autograder_score - 1} points remain)")
        test2_final_score -= 1.0
        if test2_final_score < 0:  # score cannot be negative
            test2_final_score = 0.0

    output += f"Private test_final_score: {test2_final_score}\n"
    print(f"You received {test_score} out of {test2_max_score} in the private tests")
    print(f"Private test final_score: {test2_final_score} out of {test2_autograder_score}")
    # ============================================================

    # ============================================================
    # Preprare gradescope result
    final_score = test1_final_score + test2_final_score
    print(f"You received {test1_final_score} points out of {test1_autograder_score} in the public tests")
    print(f"You received {test2_final_score} points out of {test2_autograder_score} in the private tests")
    print(f"The final score is {final_score}.")


    result = {}
    result["score"] = final_score
    result["output"] = run_proc_stdout + "\n" + run_proc_stdout_test1
    result["extra_data"] = {}
    result["stdout_visibility"] = "visible"
    

    # ============================================================
    # Clean up
    output += "Clean up keyspace...\n"
    run("/autograder/source/after_run_autograder.sh")
    # ============================================================

    return result


def capture_failed_test(output):
    lines = output.split("\n")
    for i, line in enumerate(lines):
        # print(f"line {i}: {line}")
        if "Grader.test" in line:
            # extract the test name
            test_name = line.split("Grader.")[1].split("(")[0]
            # extract the test number
            test_num = int(test_name.split("_")[0].split("test")[1])
            return test_name, test_num
    return None, None

def capture_exception(output):
    keywords = [
        "Exception",
        "exception"
    ]
    lines = output.split("\n")
    for line in lines:
        for keyword in keywords:
            if keyword in line:
                return line
    return None


def filter_unwanted_messages(output):
    unwanted_patterns = [
        "WARNING"
        "Netty Epoll Transport",
        "Platform Low-Level API",
        "INFO",
        "NIOTransport",
        "handleMessageFromClient",
        "handleMessageFromServer",
        "broadcastRequest"
    ]

    lines = output.split("\n")
    filtered_lines = [line for line in lines if not any(pattern in line for pattern in unwanted_patterns)]
    return "\n".join(filtered_lines)




# def run(cmd, capture_output=False, timeout=None):
#     stdout = subprocess.PIPE if capture_output else None
#     stderr = subprocess.PIPE if capture_output else None

#     try:
#         proc = subprocess.run(cmd, cwd=SUBMISSION_DIR, shell=True, stdout=stdout, stderr=stderr, timeout=timeout)
#     except subprocess.TimeoutExpired:
#         print(f"Process timed out after {timeout} seconds")
#         return subprocess.CompletedProcess(cmd, -1, stdout=b'', stderr=b'Process timed out')

#     if capture_output:
#         proc.stdout = filter_unwanted_messages(proc.stdout.decode('utf-8')).encode('utf-8')
#         proc.stderr = filter_unwanted_messages(proc.stderr.decode('utf-8')).encode('utf-8')
#     return proc

def run(cmd, capture_output=False, timeout=None):
    # Define paths for output files
    stdout_path = os.path.join(SUBMISSION_DIR, "stdout.txt")
    stderr_path = os.path.join(SUBMISSION_DIR, "stderr.txt")

    # Remove the previous output files if they exist
    if capture_output:
        if os.path.exists(stdout_path):
            os.remove(stdout_path)
        if os.path.exists(stderr_path):
            os.remove(stderr_path)

    # Open file handles for stdout and stderr
    stdout_file = open(stdout_path, 'w+') if capture_output else None
    stderr_file = open(stderr_path, 'w+') if capture_output else None

    proc = None
    try:
        # Run the process and capture the output in files
        proc = subprocess.run(cmd, cwd=SUBMISSION_DIR, shell=True, stdout=stdout_file, stderr=stderr_file, timeout=timeout)
    except subprocess.TimeoutExpired:
        print(f"Process timed out after {timeout} seconds")

    # Close file handles if they were opened
    if stdout_file:
        stdout_file.close()
    if stderr_file:
        stderr_file.close()

    # Check if the process has completed; if not, create a CompletedProcess object
    if proc is None:
        proc = subprocess.CompletedProcess(cmd, -1, stdout='', stderr='Process timed out')

    # Read the contents of the output files
    if capture_output:
        with open(stdout_path, 'r') as f:
            proc.stdout = f.read()
        with open(stderr_path, 'r') as f:
            proc.stderr = f.read()

    # Apply any filtering to the output
    if capture_output:
        proc.stdout = filter_unwanted_messages(proc.stdout).encode('utf-8')
        proc.stderr = filter_unwanted_messages(proc.stderr).encode('utf-8')

    return proc






# def run(cmd, capture_output=False):
#     stdout = subprocess.PIPE if capture_output else None
#     stderr = subprocess.PIPE if capture_output else None

#     proc = subprocess.run(cmd, cwd=SUBMISSION_DIR, shell=True, stdout=stdout, stderr=stderr)
#     if capture_output:
#         proc.stdout = filter_unwanted_messages(proc.stdout.decode('utf-8')).encode('utf-8')
#         proc.stderr = filter_unwanted_messages(proc.stderr.decode('utf-8')).encode('utf-8')
#     return proc

def output_json(data):
    with open(END_RESULTS, "w") as f:
        json.dump(data, f, sort_keys=True, indent=4, separators=(',', ': '))


if __name__ == "__main__":
    try:
        result = main()
    except SubmissionError as err:
        result = {}
        result["score"] = 0.0
        result["output"] = err.msg
        result["extra_data"] = {}
        result["extra_data"]["extra_msg"] = err.extra_msg
        result["extra_data"]["err_tb"] = traceback.format_exc()
        result["stdout_visibility"] = "visible"
    except Exception as err:
        result = {}
        result["score"] = 0.0
        result["output"] = UNEXPECTED
        result["extra_data"] = {}
        result["extra_data"]["err_msg"] = str(err)
        result["extra_data"]["err_tb"] = traceback.format_exc()
        result["stdout_visibility"] = "visible"

    try:
        output_json(result)
        print()
        print(END_RESULTS)
        print()
        with open(END_RESULTS) as f:
            print(f.read())
    except:
        traceback.print_exc()
