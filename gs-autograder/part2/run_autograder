#!/usr/bin/env python3

import json
import subprocess
import traceback
from datetime import datetime
import os.path as path
import os
import glob
import shutil
import re

# error message to show to student when an unexpected error has occurred
UNEXPECTED = "unexpected error; contact the TA with a link to this page"

SUBMISSION_DIR = "/autograder/submission"
SOURCE_DIR = "/autograder/source"
RESULTS_DIR = "/autograder/results"
END_RESULTS = f"{RESULTS_DIR}/results.json"

TEST1_PASS = False
TEST_TIMEOUT = False
TEST2_PASS = False


class SubmissionError(Exception):
    def __init__(self, msg, extra_msg=""):
        self.msg = msg
        self.extra_msg = extra_msg


def main():
    global TEST1_PASS
    global TEST_TIMEOUT
    global TEST2_PASS

    print(datetime.now())
    print()
    print("begin setup")
    print()

    # Check if the required files exist
    output = ""
    output += "Expecting to find the following files:\n"

    client_exists = path.exists(f"{SUBMISSION_DIR}/src/client/MyDBClient.java")
    output += "src/client/MyDBClient.java ... "
    output += "found" if client_exists else "NOT FOUND"
    output += "\n"

    server_exists = path.exists(
        f"{SUBMISSION_DIR}/src/server/MyDBSingleServer.java")
    output += "src/server/MyDBSingleServer.java ... "
    output += "found" if server_exists else "NOT FOUND"
    output += "\n"

    server_exists = path.exists(
        f"{SUBMISSION_DIR}/src/server/MyDBReplicatedServer.java")
    output += "src/server/MyDBReplicatedServer.java ... "
    output += "found" if server_exists else "NOT FOUND"
    output += "\n"

    document_exists = path.exists(f"{SUBMISSION_DIR}/Design.pdf")
    output += "Design.pdf ... "
    output += "found" if document_exists else "NOT FOUND"
    output += "\n"

    if not (client_exists and server_exists):
        raise SubmissionError(output)


    # Reset the source directory
    output += "Resetting all other files to their original form\n"
    run(f"rm {SOURCE_DIR}/framework/src/client/MyDBClient.java")
    run(f"rm {SOURCE_DIR}/framework/src/server/MyDBSingleServer.java")
    run(f"rm {SOURCE_DIR}/framework/src/server/MyDBReplicatedServer.java")
    
    run(f"mv src/client/MyDBClient.java {SOURCE_DIR}/framework/src/client/MyDBClient.java")
    run(f"mv src/server/MyDBSingleServer.java {SOURCE_DIR}/framework/src/server/MyDBSingleServer.java")
    run(f"mv src/server/MyDBReplicatedServer.java {SOURCE_DIR}/framework/src/server/MyDBReplicatedServer.java")

    try:
        for item in os.listdir(f"{SUBMISSION_DIR}/src"):
            s = os.path.join(f"{SUBMISSION_DIR}/src", item)
            d = os.path.join(f"{SOURCE_DIR}/framework/src", item)
            if os.path.isdir(s):
                shutil.copytree(s, d, dirs_exist_ok=True)
    except:
        pass


    run("rm -r *")
    run(f"cp -r {SOURCE_DIR}/framework/* .")  # copy all files from source dir to submission dir




    # Complie the code
    output += "Compiling...\n"
    print("compiling...")

    jar_files = ["lib/" + x for x in os.listdir(f"{SUBMISSION_DIR}/lib")]
    java_files = glob.glob(f"{SUBMISSION_DIR}/src/**/*.java", recursive=True)

    # remove AVDB*.java files if they exist
    java_files = [x for x in java_files if not x.startswith(f"{SUBMISSION_DIR}/src/server/AVDB")]
    java_files = [x for x in java_files if not x.startswith(f"{SUBMISSION_DIR}/src/client/AVDB")]

    test_java_files = [
    	"test/GraderSingleServer.java",
    	"test/GraderSingleServerGradescope.java",
        "test/GraderGradescope.java",
        "test/Grader.java"
        ]
    java_files += test_java_files

    
    print(f"jar_files: {jar_files}")
    print(f"java_files: {java_files}")

    compile_proc = run(
        f"javac -cp {':'.join(jar_files)} {' '.join(java_files)}",
        capture_output=True
    )
    compile_proc_stdout = compile_proc.stdout.decode('utf-8')
    compile_proc_stderr = compile_proc.stderr.decode('utf-8')

    print(compile_proc.args)
    print("compilation stdout:")
    print(compile_proc_stdout)
    print()
    print("compilation stderr:")
    print(compile_proc_stderr)
    print()

    if compile_proc.returncode != 0:
        output += "Java compilation error:\n"
        output += compile_proc_stderr
        raise SubmissionError(output)
    output += "Compilation successful\n"


    # Start the Cassandra node
    print("Starting Cassandra...")
    output += "Starting cassandra...\n"
    run("/autograder/source/before_run_autograder.sh")


    # Run the public tests
    output += "Running Public Tests...\n"
    NUN_TEST = 3
    average_pub_test_score = 0.0
    for n in range(1, NUN_TEST + 1):
        print(f"Running public unit tests... {n}-th round")
        run_proc = run(
            f"java -cp src:test:{':'.join(jar_files)} GraderGradescope",
            capture_output=True,
            timeout=90 
        )
        run_proc_stdout = run_proc.stdout.decode('utf-8')
        run_proc_stderr = run_proc.stderr.decode('utf-8')

        output += f"Public test stdout at round {n}:\n"
        output += run_proc_stdout
        output += f"Public test stderr at round {n}:\n"
        output += run_proc_stderr
        # print(run_proc.args)
        # print(run_proc_stdout)
        # print()
        # print("runtime stderr:")
        # print(filter_unwanted_messages(run_proc_stderr))
        # print()

        # Parse stdout to get test names and their max scores
        TestNameAndScore = {}
        for line in run_proc_stdout.strip().split("\n"):
            match = re.search(r"testName=(\w+)\smax_score=([\d\.]+)", line)
            if match:
                test_name = match.group(1)
                max_score = float(match.group(2))
                TestNameAndScore[test_name] = {"max_score": max_score}
        TestNameAndScore = dict(sorted(TestNameAndScore.items(), key=lambda x: x[0]))

        test_score = 0.0
        test1_autograder_score = 25.0
        test1_max_score = 0.0

        # if a timeout occurs 
        if run_proc.returncode == -1 and TEST_TIMEOUT is not True:
            print()
            print("A timeout has occurred when running the tests.")
            print("Deducting 1 point of total score for TIMEOUT error (probably not terminating cleanly).")
            print()
            TEST_TIMEOUT = True
        
        # Get the total max score
        for test_name, test_info in TestNameAndScore.items():
            test1_max_score += test_info["max_score"]
        if test1_max_score == 0:
            print("The grader crashed. Please refer to the following stderr output:")
            print(run_proc_stderr)
            print()
            print(f"You received {0} out of {test1_autograder_score} at the private tests at the {n}-th round")
            print()
            continue

        # If any error occurs
        if run_proc.returncode != 0: 
            output += "Java runtime error\n"
            # raise SubmissionError(output)
            failed_test_name, failed_test_num = capture_failed_test(run_proc_stderr)
            print(f"The submission failed at : {failed_test_name}")
            print("The tests you passed:")
            # Sum up the scores before the failed test 
            for test_name, test_info in TestNameAndScore.items(): 
                if test_name == failed_test_name:
                    break
                received_score = test_info["max_score"]
                print(f"testname: {test_name}; get {received_score } points")
                test_score += received_score 
        else:
            print("All tests pass!")
            failed_test_name = None
            failed_test_num = None
            TEST1_PASS = True
            test_score = test1_max_score

        test1_final_score = (test_score / test1_max_score) * test1_autograder_score

        # If any exception occurs
        if capture_exception(run_proc_stderr):
            output += "Exception:\n"
            output += capture_exception(run_proc_stderr)
            output += "\n"
            print("Stderr output indicates unexpected exceptions (e.g., ArrayOutofBounds, NullPointer or others) because of inadequate checks in code.")
            print(f"Lose 1 point out of {test1_autograder_score}; {test1_autograder_score - 1} points remain at the public test round {n}")
            test1_final_score -= 1.0
            if test1_final_score < 0:  # score cannot be negative
                test1_final_score = 0.0

        average_pub_test_score += test1_final_score
        print(f"You received {test1_final_score} out of {test1_autograder_score} at the public tests at the {n}-th round")
    
    average_pub_test_score /= NUN_TEST
    print(f"The average score of the public tests (acorss 3 rounds) is {average_pub_test_score} out of {test1_autograder_score}")










    # Run the private tests
    output += "Running Private Tests...\n"
    print("Running private unit tests...")

    NUN_TEST = 3
    average_pri_test_score = 0.0
    for n in range(1, NUN_TEST + 1):

        print(f"Running private unit tests... {n}-th round")
        run_proc = run(
            f"java -cp src:test:{':'.join(jar_files)} -Dconfig={SOURCE_DIR}/framework/conf/servers_20.properties GraderGradescope",
            capture_output=True,
            timeout=90 
        )
        run_proc_stdout = run_proc.stdout.decode('utf-8')
        run_proc_stderr = run_proc.stderr.decode('utf-8')

        output += f"Private test stdout at round {n}:\n"
        output += run_proc_stdout
        output += f"Private test stderr at round {n}:\n"
        output += run_proc_stderr
        # print()
        # print("runtime stderr:")
        # print(filter_unwanted_messages(run_proc_stderr))
        # print()

        TestNameAndScore = {}
        for line in run_proc_stdout.strip().split("\n"):
            match = re.search(r"testName=(\w+)\smax_score=([\d\.]+)", line)
            if match:
                test_name = match.group(1)
                max_score = float(match.group(2))
                TestNameAndScore[test_name] = {"max_score": max_score}
        TestNameAndScore = dict(sorted(TestNameAndScore.items(), key=lambda x: x[0]))

        test_score = 0.0
        test2_autograder_score = 30.0
        test2_max_score = 0.0

        # if a timeout occurs 
        if run_proc.returncode == -1 and TEST_TIMEOUT is not True:
            print()
            print("A timeout has occurred when running the tests.")
            print("Deducting 1 point of total score for TIMEOUT error (probably not terminating cleanly).")
            print()
            TEST_TIMEOUT = True
        
        # Get the total max score in this test
        for test_name, test_info in TestNameAndScore.items():
            test2_max_score += test_info["max_score"]
        if test2_max_score == 0:
            print("The grader crashed. Please refer to the following stderr output:")
            print(run_proc_stderr)
            print()
            print(f"You received {0} out of {test2_autograder_score} at the private tests at the {n}-th round")
            print()
            continue

            

        # If any error occurs
        if run_proc.returncode != 0: 
            output += "Java runtime error\n"
            failed_test_name, failed_test_num = capture_failed_test(run_proc_stderr)
            print(f"The submission failed at : {failed_test_name}")
            print("The tests you passed:")
            # Sum up the scores before the failed test 
            for test_name, test_info in TestNameAndScore.items(): 
                if test_name == failed_test_name:
                    break
                received_score = test_info["max_score"]
                print(f"testname: {test_name}; get {received_score } points")
                test_score += received_score 
        
        # If all tests pass
        else:
            print("All tests pass!")
            failed_test_name = None
            failed_test_num = None
            TEST2_PASS = True
            test_score = test2_max_score

        test2_final_score = (test_score / test2_max_score) * test2_autograder_score

        # If any exception occurs
        if capture_exception(run_proc_stderr):
            output += "Exception:\n"
            output += capture_exception(run_proc_stderr)
            output += "\n"
            print()
            print("Stderr output indicates unexpected exceptions (e.g., ArrayOutofBounds, NullPointer or others) because of inadequate checks in code.")
            print(f"Lose 1 point out of {test2_autograder_score}; {test2_autograder_score - 1} points remain at the private test round {n}")
            print()
            test2_final_score -= 1.0
            if test2_final_score < 0:  # score cannot be negative
                test2_final_score = 0.0

        average_pri_test_score += test2_final_score
        print()
        print(f"You received {test2_final_score} out of {test2_autograder_score} at the private tests at the {n}-th round")
        print()
    
    average_pri_test_score /= NUN_TEST
    print()
    print(f"The average score of the private tests (acorss 3 rounds) is {average_pri_test_score} out of {test2_autograder_score}")
    print()






    # Prepare the autograder result
    final_score = average_pub_test_score + average_pri_test_score 
    
    if TEST_TIMEOUT:  # If timeout, deduct 1 point
        print(f"Deducting 1 point out of {final_score} due to TIMEOUT error.")
        final_score -= 1.0

    final_score = final_score * 50 / 55  # scale to 50 points

    
    print("Summary:")
    print(f"You received {average_pub_test_score} points out of {test1_autograder_score} at the public tests")
    print(f"You received {average_pri_test_score } points out of {test2_autograder_score} at the private tests")
    print(f"After scaling to the maximum autograder score (55 -> 50), the final score is {final_score}.")



    result = {}
    result["score"] = final_score
    result["output"] = output
    result["extra_data"] = {}
    result["stdout_visibility"] = "visible"
    

    # ============================================================
    # # Clean up
    # output += "Clean up keyspace...\n"
    # run("/autograder/source/after_run_autograder.sh")
    # ============================================================

    return result


def capture_failed_test(output):
    lines = output.split("\n")
    for i, line in enumerate(lines):
        # print(f"line {i}: {line}")
        if "Grader.test" in line:
            # extract the test name
            test_name = line.split("Grader.")[1].split("(")[0]
            # extract the test number
            test_num = int(test_name.split("_")[0].split("test")[1])
            return test_name, test_num
    return None, None

def capture_exception(output):
    keywords = [
        "Exception",
        "exception"
    ]
    lines = output.split("\n")
    for line in lines:
        for keyword in keywords:
            if keyword in line:
                return line
    return None


def filter_unwanted_messages(output):
    unwanted_patterns = [
        "WARNING",
        "warning"
    ]

    lines = output.split("\n")
    filtered_lines = [line for line in lines if not any(pattern in line for pattern in unwanted_patterns)]
    return "\n".join(filtered_lines)




# def run(cmd, capture_output=False, timeout=None):
#     stdout = subprocess.PIPE if capture_output else None
#     stderr = subprocess.PIPE if capture_output else None

#     try:
#         proc = subprocess.run(cmd, cwd=SUBMISSION_DIR, shell=True, stdout=stdout, stderr=stderr, timeout=timeout)
#     except subprocess.TimeoutExpired:
#         print(f"Process timed out after {timeout} seconds")
#         return subprocess.CompletedProcess(cmd, -1, stdout=b'', stderr=b'Process timed out')

#     if capture_output:
#         proc.stdout = filter_unwanted_messages(proc.stdout.decode('utf-8')).encode('utf-8')
#         proc.stderr = filter_unwanted_messages(proc.stderr.decode('utf-8')).encode('utf-8')
#     return proc

def run(cmd, capture_output=False, timeout=None):
    # Define paths for output files
    stdout_path = os.path.join(SUBMISSION_DIR, "stdout.txt")
    stderr_path = os.path.join(SUBMISSION_DIR, "stderr.txt")

    # Remove the previous output files if they exist
    if capture_output:
        if os.path.exists(stdout_path):
            os.remove(stdout_path)
        if os.path.exists(stderr_path):
            os.remove(stderr_path)

    # Open file handles for stdout and stderr
    stdout_file = open(stdout_path, 'w+') if capture_output else None
    stderr_file = open(stderr_path, 'w+') if capture_output else None

    proc = None
    try:
        # Run the process and capture the output in files
        proc = subprocess.run(cmd, cwd=SUBMISSION_DIR, shell=True, stdout=stdout_file, stderr=stderr_file, timeout=timeout)
    except subprocess.TimeoutExpired:
        print(f"Process timed out after {timeout} seconds")

    # Close file handles if they were opened
    if stdout_file:
        stdout_file.close()
    if stderr_file:
        stderr_file.close()

    # Check if the process has completed; if not, create a CompletedProcess object
    if proc is None:
        proc = subprocess.CompletedProcess(cmd, -1, stdout='', stderr='Process timed out')

    # Read the contents of the output files
    if capture_output:
        with open(stdout_path, 'r') as f:
            proc.stdout = f.read()
        with open(stderr_path, 'r') as f:
            proc.stderr = f.read()

    # Apply any filtering to the output
    if capture_output:
        proc.stdout = filter_unwanted_messages(proc.stdout).encode('utf-8')
        proc.stderr = filter_unwanted_messages(proc.stderr).encode('utf-8')

    return proc






# def run(cmd, capture_output=False):
#     stdout = subprocess.PIPE if capture_output else None
#     stderr = subprocess.PIPE if capture_output else None

#     proc = subprocess.run(cmd, cwd=SUBMISSION_DIR, shell=True, stdout=stdout, stderr=stderr)
#     if capture_output:
#         proc.stdout = filter_unwanted_messages(proc.stdout.decode('utf-8')).encode('utf-8')
#         proc.stderr = filter_unwanted_messages(proc.stderr.decode('utf-8')).encode('utf-8')
#     return proc

def output_json(data):
    with open(END_RESULTS, "w") as f:
        json.dump(data, f, sort_keys=True, indent=4, separators=(',', ': '))


if __name__ == "__main__":
    try:
        result = main()
    except SubmissionError as err:
        result = {}
        result["score"] = 0.0
        result["output"] = err.msg
        result["extra_data"] = {}
        result["extra_data"]["extra_msg"] = err.extra_msg
        result["extra_data"]["err_tb"] = traceback.format_exc()
        result["stdout_visibility"] = "visible"
    except Exception as err:
        result = {}
        result["score"] = 0.0
        result["output"] = UNEXPECTED
        result["extra_data"] = {}
        result["extra_data"]["err_msg"] = str(err)
        result["extra_data"]["err_tb"] = traceback.format_exc()
        result["stdout_visibility"] = "visible"

    try:
        output_json(result)
        print()
        print(END_RESULTS)
        print()
        with open(END_RESULTS) as f:
            print(f.read())
    except:
        traceback.print_exc()


